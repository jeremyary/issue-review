# GitHub Personal Access Token with 'repo' and 'public_repo' scopes
GITHUB_TOKEN=your_github_token_here

# LLM Configuration (OpenAI-compatible API)
# Works with: Anthropic, OpenAI, local vLLM/Ollama, any OpenAI-compatible endpoint
#
# Anthropic Claude:
#   LLM_BASE_URL=https://api.anthropic.com/v1/
#   LLM_API_KEY=sk-ant-...
#   LLM_MODEL=claude-sonnet-4-5
#
# OpenAI:
#   LLM_BASE_URL=https://api.openai.com/v1/
#   LLM_API_KEY=sk-...
#   LLM_MODEL=gpt-4o
#
# Local vLLM:
#   LLM_BASE_URL=http://localhost:8000/v1/
#   LLM_API_KEY=
#   LLM_MODEL=meta-llama/Llama-3.1-8B-Instruct
#
# Ollama:
#   LLM_BASE_URL=http://localhost:11434/v1/
#   LLM_API_KEY=
#   LLM_MODEL=llama3.1:8b
#
LLM_BASE_URL=https://api.anthropic.com/v1/
LLM_API_KEY=sk-ant-...
LLM_MODEL=claude-sonnet-4-5

# LangFuse (optional - for observability/tracing)
# LANGFUSE_PUBLIC_KEY=pk-lf-...
# LANGFUSE_SECRET_KEY=sk-lf-...
# LANGFUSE_HOST=https://cloud.langfuse.com

# PostgreSQL with pgvector (for RAG content indexing)
# Local: docker-compose up postgres
DATABASE_URL=postgresql://issue_review:issue_review@localhost:5432/issue_review

# Catalog staleness threshold (days)
# Catalog auto-syncs if older than this when running analysis
# CATALOG_STALE_DAYS=7
